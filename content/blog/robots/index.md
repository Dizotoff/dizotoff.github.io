---
title: Robots, AI and Ethics
date: "2019-11-25"
description: "A look into ethics of AI"
---

Artificial intelligence (AI) and autonomous systems (AS) are becoming heavily integrated into our society. The development of technology and growing government interest in AI/AS, is pushing the development rapidly despite the important concerns which arise between academia and society. Some scientists predict that it would be possible to create AI which would be at the same level or much more intelligent than humans. That raises a lot of questions about AI ethics. Currently, we have only two ways of developing AI. The first option is based on a neural network or directed evolution. This approach has no transparency in it. This means nobody could possibly know why the algorithm did one thing or another. The second approach based on a decision tree or Bayesian network, and is much more transparent for a creator to examine. Bostrom and Yudkowsky declare that AI should be transparent in order to be used in one domain or another. Otherwise, if AI was put into the position of decision making like bank mortgage approval, for example, it would be not possible to understand why one was approved and another rejected [1, 9]. In addition to transparency, predictability, responsibility, auditability, and incorruptibility are necessary. Self-driving cars will be broadly introduced to society in 2025. There are two main concerns about this. The first is liability, who is going to be responsible for the action of the car? Driver, manufacturer, or both, if the driving process is a cooperation of driver and car? The second question is the trolley dilemma; Should a car ever sacrifice a passenger? Also, it is not obvious if the behaviour of the car could be set by the user or not. Despite technological challenges which come with AI/AS, it also brings a lot of ethical concerns which require the cooperation of people from different fields to solve forthcoming problems. In order to safely implement these technologies, society needs to think about potential consequences now. The challenges we face with the current technological development levels are only the tip of the iceberg.

Artificial Narrow Intelligence (ANI) is AI created to perform in a domain specific area. Sometimes called Weak AI. ANI cannot function in any other field except the one it made for. A good example of area-specific AI would be the world’s best chess, checkers, go players; a smartphone; information systems for manufacturing, military, finance, medicine; or self-driving cars. It is obvious that ANI systems are already widely spread across different parts of our life. Even though, it is relatively easy to think about ethics and morale in these domains, implementation of new ANI systems sometimes creates huge ethical concerns.
Malle says that ANI should always follow human commands, because they aren’t sophisticated enough to recognize human error [3, 245]. These robots won't have a free will, which means that manufacturers will hold all the responsibility of failure, assuming the final product cannot be programmed by the end-user. However, Malle doesn’t say anything about the transparency of ANI systems. These systems can sometimes be programmed in a way that excludes the possibility for examination into the reasons for possible outcomes. Meaning, if a system behaves in a certain way, it is impossible to understand exactly why. That could create additional challenges when deploying such systems into decision-making areas. Bostrom and Yudkowsky propose an approach based on decision tree or Bayesian networks when designing ANI systems to create transparency for investigation systems [1,1].
Nevertheless, it is predictable that self-driving cars will be largely introduced to our lives very soon. Problem is, that it's not obvious how the car should behave in complicated situations like sacrificing couple of pedestrians or the driver. Faulhaber and Dittmer suggest that the best way to implement ethics into a self-driving car is to program cars with self-sacrificing utilitarian ethics [2,12]. This approach should be implemented in all cars, to ensure cars with different settings do not exist in traffic. Otherwise, it could create even more damage. Cars with different behavior setting are dangerous, that's why the implementation of a widely used common standard, at the government level is needed. Others suggest that it should be up to the end-user which setting to chose. In this case, a manufacturer will have less liability, but there may be more accidents as conflicting programs compete. According to the research, people, when driving a car in VR, act in favor of the quantitative greater good. Context and age modulate this pattern, but it also applies even in the case of self-sacrifice.

Artificial General Intelligence (AGI) is generally applicable AI which is similar to human brain capacities. Creating AGI is a much harder task than creating ANI, and we’re yet to do it. It's not created for the specific domain, but instead can do different things by gathering pieces of information from several domains. It's a unique skill available only to humans [1, 3]. These systems will have the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, and learn fast from own experience as easily as humans can. It is important to mention that once possible; all these activities could be performed by AGI much faster than a human.
Malle says that to implement moral competence into fully autonomous systems we need to understand first how it works in humans’ brain. He suggests a framework of five elements which make up our moral competence: moral cognition; moral decision; moral communication; a system of norms; moral vocabulary [3,245]. Humans’ moral cognition is the process of detecting and evaluating norm-violating acts. An interesting thing about moral cognition is that it is divided into two separate branches depending on if the norm- violating act is intentional or unintentional. If it’s intentional, humans start searching for a motive. If it’s unintentional, our competence system looks to see if there was a possibility of preventing the event. Moral vocabulary is a set of norms and properties, norms violations, and responses to these violations. Human moral vocabulary is extremely rich , so Malle proposes to seed robots with only main ones and then link others using an approximation to them. Moral communication is a possibility of a moral competent agent to express judgement to the norm violators. Problem with it is that humans tend to treat robot as lower in status and expect them to not voice their moral criticism [3,251]. On the other hand, robots with good deployed language skills won't have corresponding anger and outrage while expressing their opinion. Moral decision making and action is about what makes the robot behave morally. Malle says that a robot’s behaviour should be vulnerable to blame [3,249]. As mentioned earlier, blame used as a pedagogical tool which provides a robot with reasons not to violate the norm again. Malle says that this sort of capacity to learn and adjust one’s choices is needed, not a metaphysical one.
Malle has a psychology background and it is easy to see that he comes to the problem from a very psychological angle with a lack of technological knowledge. Malle’s statement creates two problems: defining what really is the human moral competence and then implementing it. Instead of trying to implement humans moral competence into AGI/AS, Bostrom and Yudkowsky suggest that to build the AGI/AS to act safely, it needs to be able to predict possible consequences. One must specify good behavior in such terms as “X such that the consequence of X is not harmful to humans” [1,4]. Bostrom and Yudkowsky are more concerned about the safety of these systems than about their high moral competence. They think that it is impossible to predict local, specific behaviour of AI, apart from its safety. But they declare that ethical cognition itself must be taken as a subject matter of engineering.

A lot of ethical problems are presented to us right now by current AI/AS. The introduction of these technologies into military, education, transportation, or medicine is inevitable and going to be challenging. Self-driving car dilemmas create a lot of issues and different opinions. Governments are pushing the development by investing heavily in development of AI/AS powered weapon systems. But what's going to be even more challenging is what comes with the development of more advanced algorithms similar to humans’ abilities. Social roles could be filled with them, which creates far more complicated difficulties. The problem of how we should treat such systems rises with the growth of their moral competence. Concerns about responsibility for the actions of such systems and introduction of free will. All these questions are up for discussion and debate between people from a number of different fields. Hopefully, all together we can create a safe future for humanity.

References

1. Bostrom Nick and Eliezer Yudkowsky. The Ethics of Artificial Intelligence. In Cambridge Handbook of Artificial Intelligence; New York: Cambridge University Press, 2018: pp.1-21

2. Anja K. Faulhaber, Anke Dittmer, Felix Blind, Maximilian A. Wachter, Silja Timm, Leon R. Sütfeld, Achim Stephan, Gordon Pipa, Peter König. Human decisions in moral dilemmas are largely described by utilitarianism: Virtual car driving study provides guidelines for autonomous driving vehicles; Science and engineering ethics, 2018: pp.1-20.

3. Bertram F. Malle, Integrating robot ethics and machine morality: the study and design of moral competence in robots Ethics and Information Technology, 2016: pp.243-256.
